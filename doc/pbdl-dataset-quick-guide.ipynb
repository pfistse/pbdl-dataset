{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc13ef-6cc1-444b-8633-41450fb58308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85d697-7f1b-4d2f-8b92-ad3d0fca0407",
   "metadata": {},
   "source": [
    "# A short example: transonic cylinder flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85744acb-ca28-46f8-9f8b-2ebc4f36eadf",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2ec59-c418-49c5-9830-0d561906b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbdl.torch.loader import *\n",
    "import examples.tcf.net_small as net_small\n",
    "\n",
    "dataset = Dataset(\n",
    "    \"transonic-cylinder-flow-tiny\", # dataset name\n",
    "    time_steps=10, # time steps between input and target frame\n",
    "    sel_sims=[0,1], # use only the first two simulations\n",
    "    step_size=3, # trim_start=100, trim_end=100,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "loader = Dataloader(dataset, # batch_sampler=...,\n",
    "    batch_size=3, shuffle=True)\n",
    "\n",
    "net = net_small.NetworkSmall()\n",
    "criterionL2 = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481ad3a-1fb2-4ed3-a452-ab8e07d1c582",
   "metadata": {},
   "source": [
    "A PBDLDataset instance provides the data samples to the data loader and must be initialized with the dataset name and the time steps between input and target sample. Due to the specification of `sel_sims`, only the first two simulations are used. As the transonic-cylinder-flow dataset is quite large for quick training, it is thinned out using the parameter `step_size` (every third sample is used). Additionally, you could use `trim_start` and `trim_end` to discard a (possibly uninteresting) start/end sequence of samples. Optionally, it is possible to specify whether the data should be normalized.\n",
    "\n",
    "A PBDLDataLoader instance takes the dataset as input and prepares the samples for later training with the CNN (e.g. blows up the constants into layers). A loader instance is iterable, returning batches of the specified `batch_size`. When integrating solvers into deep learning, it may be required for samples in a batch to have the same constants; this is ensured by the PBDLConstantBatchSampler (not used in this example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836f570-a584-425d-bd38-0e0902c9d6a0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4b4ef-47a7-4086-acce-480fcb2d1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    for i, (input, target) in enumerate(loader):\n",
    "\n",
    "        net.zero_grad()\n",
    "        output = net(input)\n",
    "\n",
    "        loss = criterionL2(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"epoch { epoch }, loss { loss.item() }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c50a3-bb1f-4f18-b0de-0b26e9da58e3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e11f81-a02e-4043-a5db-31971d163f0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "input, target = next(iter(loader))\n",
    "output = net(input)\n",
    "\n",
    "input = input.numpy()\n",
    "target = target.numpy()\n",
    "output = output.detach().numpy()\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(np.flip(input[0, 1, ...], axis=-2), cmap=\"magma\")\n",
    "plt.title(\"Input\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(np.flip(output[0, 1, ...], axis=-2), cmap=\"magma\")\n",
    "plt.title(\"Output\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(np.flip(target[0, 1, ...], axis=-2), cmap=\"magma\")\n",
    "plt.title(\"Target\")\n",
    "\n",
    "diff = target[0, 1, ...] - output[0, 1, ...]\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(np.flip(diff, axis=-2), cmap=\"gray\")\n",
    "plt.title(\"Difference\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423157e-173f-4cc8-a5ba-880bae0d34ca",
   "metadata": {},
   "source": [
    "# Comprehensive example: solver-in-the-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a28d0-db53-4104-b151-3df48edd7211",
   "metadata": {},
   "source": [
    "The following example shows how to use\n",
    "* the PBDLConstantBatchSampler\n",
    "* PyTorch-PhiFlow tensor conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735b118-ec1a-42fd-9532-2795d2893b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbdl.torch.phi.loader import *\n",
    "from examples.ks.ks_networks import ConvResNet1D\n",
    "from examples.ks.ks_solver import DifferentiableKS\n",
    "\n",
    "# training parameters\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "EPOCHS = 4\n",
    "\n",
    "# solver parameters\n",
    "RES = 48\n",
    "TIMESTEP = 0.5\n",
    "DOMAIN_SIZE_BASE = 8\n",
    "PREDHORZ = 5\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "diff_ks = DifferentiableKS(resolution=RES, dt=TIMESTEP)\n",
    "\n",
    "dataset = Dataset(\n",
    "    \"ks-dataset\",\n",
    "    time_steps=PREDHORZ,\n",
    "    step_size=20,\n",
    "    intermediate_time_steps=True,\n",
    "    normalize=False,\n",
    ")\n",
    "\n",
    "batch_sampler = ConstantBatchSampler(dataset, BATCH_SIZE, group_constants=[0]) # group after first constant\n",
    "dataloader = Dataloader(dataset, batch_sampler=batch_sampler)\n",
    "\n",
    "net = ConvResNet1D(16, 3, device=device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb215c2-5ecd-4840-bc5d-777773b41bb3",
   "metadata": {},
   "source": [
    "Notice that the dataset is initialized with the `intermediate_time_steps` flag. The dataset now not only supplies the initial and the target frame, but also all frames in between.\n",
    "\n",
    "We also use the PBDLConstantBatchSampler to ensure that all samples in a batch have the same constants. This is necessary because for each batch only one domain size can be passed to the solver function. You can use `group_constants` to specify which constants should be considered for grouping into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1cad5-9d76-473b-a02f-f4aa1b49a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for i, (input, targets) in enumerate(dataloader):\n",
    "\n",
    "        input = input.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        domain_size = input[0][1][0].item()\n",
    "\n",
    "        inputs = [input]\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(PREDHORZ):\n",
    "            output_solver = diff_ks.etd1(\n",
    "                dataset.to_phiflow(inputs[-1]), DOMAIN_SIZE_BASE * domain_size\n",
    "            )\n",
    "\n",
    "            correction = diff_ks.dt * net(inputs[-1])\n",
    "            output_combined = dataset.from_phiflow(output_solver) + correction\n",
    "\n",
    "            outputs.append(output_combined)\n",
    "            inputs.append(dataset.cat_constants(outputs[-1], inputs[0]))\n",
    "\n",
    "        outputs = torch.stack(outputs, axis=1)\n",
    "\n",
    "        loss_value = loss(outputs, targets)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"epoch { epoch }, loss {(loss_value.item()*10000.) :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08407f9f-b325-43cb-b33b-337a3f8b5323",
   "metadata": {},
   "source": [
    "When a solver is included in the training process, it is necessary to convert between PyTorch tensors and solver tensors and possibly add or remove constant layers. For this purpose, the PBDLDataset class provides the auxiliary methods `to_phiflow`, `from_phiflow` and `cat_constants` to convert between tensor types and to add constant layers to the solver output. `cat_constants` function takes a reference tensor as input, from which it copies the constant layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7474fa-7e05-416a-bff9-dac45d8f0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, targets = next(iter(dataloader))\n",
    "\n",
    "domain_size = input[0][1][0].item()\n",
    "\n",
    "inputs = [input]\n",
    "outputs = []\n",
    "\n",
    "for _ in range(PREDHORZ):\n",
    "    output_solver = diff_ks.etd1(\n",
    "        dataset.to_phiflow(inputs[-1]), DOMAIN_SIZE_BASE * domain_size\n",
    "    )\n",
    "    output_combined = dataset.from_phiflow(output_solver) + diff_ks.dt * net(inputs[-1])\n",
    "\n",
    "    outputs.append(output_combined)\n",
    "    inputs.append(dataset.cat_constants(outputs[-1], inputs[0]))\n",
    "\n",
    "outputs = torch.stack(outputs, axis=1)\n",
    "\n",
    "input = inputs[0][0][0:1, ...].detach().numpy()\n",
    "output = outputs[0][-1].detach().numpy()\n",
    "target = targets[0][-1]\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.imshow(input, cmap=\"magma\", aspect=1)\n",
    "plt.title(\"input\")\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.imshow(output, cmap=\"magma\", aspect=1)\n",
    "plt.title(\"output\")\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.imshow(target, cmap=\"magma\", aspect=1)\n",
    "plt.title(\"target\")\n",
    "\n",
    "diff = target - output\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.imshow(diff, cmap=\"gray\", aspect=1)\n",
    "plt.title(\"target-output difference\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6d57d-d173-4a77-8c51-0d6ba3f4481c",
   "metadata": {},
   "source": [
    "# Dataset files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d224fe-856c-46dc-a4af-ff7eb2c6fe7f",
   "metadata": {},
   "source": [
    "A `.hdf5`-file is just a hierarchically structured collection of arrays (see [official documentation](https://docs.h5py.org/en/stable/quick.html)). For pbdl datasets, the hierarchy is very flat &ndash; there is just one group `sim/` containing all simulations (numpy arrays). These arrays are named `sim` concatenated with an incremental index. The `.hdf5` file format supports attaching metadata to arrays; for pbdl datasets this is used to store the constants.\n",
    "\n",
    "Each simulation array has the shape `(frame, field, spatial dims...)`. The attached constant array has the shape `(const)` for single-file datasets and `[const]` for partitioned datasets. The following code shows how to create the dataset file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70799d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "sim = np.zeros((1000, 3, 64, 32))\n",
    "const = np.zeros((2))\n",
    "\n",
    "with h5py.File(\"mydataset.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(\"sims/sim0\", data=sim)\n",
    "    dset.attrs[\"const\"] = const\n",
    "\n",
    "    # you can add more simulation array 'sims/sim1', 'sims/sim2', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb25ea-8ef5-4cc7-959a-898ce3fd01ed",
   "metadata": {},
   "source": [
    "# Local datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8edc0df-0e06-4d68-9ec9-f081fde77bd2",
   "metadata": {},
   "source": [
    "In the following example we will add a local dataset to the index, so it can be used with PBDLDataset. First we need a dataset file, which we download from LRZ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8da461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "os.makedirs(os.path.dirname(\"./local_datasets/\"), exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://syncandshare.lrz.de/dl/fiFpt1oyzXW8J4uvg43JF8/transonic_cylinder_flow_tiny.hdf5\",\n",
    "    \"./local_datasets/transonic_cylinder_flow_tiny.hdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480afbc5",
   "metadata": {},
   "source": [
    "\n",
    "PBDLDataset looks for datasets in two places: in the global index on the server and (if it exists) in the local index. The local index is a JSON file named `datasets.json` and located in the working directory. The following code creates such a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb7961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"local-transonic-cylinder-flow\": {\n",
    "        \"path\": \"./local_datasets/transonic_cylinder_flow_tiny.hdf5\",\n",
    "        \"fields\": \"VVdp\",\n",
    "        \"field_desc\" : [\"velocity x\", \"velocity y\", \"density\", \"pressure\"],\n",
    "        \"const_desc\" : [\"mach number\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"datasets.json\", \"w\") as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31b058",
   "metadata": {},
   "source": [
    "The local index file must have a specific structure: The path to your local `.npz`-file is specified by `path`. If the extension `.npz` is omitted the dataset is interpreted as a partitioned dataset. In this case, it is necessary the specify an additional attribute `num_part` for the number of partitions. `fields` contains information about the type of physical field in the form of a string, e.g. VVdp (velocity x, velocity y, density, pressure). Consecutive identical letters indicate that a physical field consists of several indices (vector field). This information affects how normalization is applied: For vector fields, the vector norm is applied first before the standard deviation is calculated.\n",
    "\n",
    "Now that the local dataset is registered, we can use it with PBDLDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9770ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pbdl.loader\n",
    "\n",
    "# package must be reloaded (not necessary in isolated code)\n",
    "importlib.reload(pbdl.loader)\n",
    "\n",
    "from pbdl.loader import *\n",
    "\n",
    "dataset = Dataset(\n",
    "    \"local-transonic-cylinder-flow\",\n",
    "    time_steps=10,\n",
    "    normalize=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
